{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "from det_rnn import *\n",
    "import det_rnn.analysis as da\n",
    "import det_rnn.train as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'D:\\proj\\det_rnn\\output\\model_200621_test'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Booster started!\n",
      "Iter.    0 | Performance -0.0081 | Loss 0.1286 | Spike loss 191.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshr\\anaconda3\\envs\\detrnn\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\joshr\\anaconda3\\envs\\detrnn\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter.   30 | Performance 0.0058 | Loss 0.0724 | Spike loss 0.7578\n",
      "Iter.   60 | Performance -0.0507 | Loss 0.0227 | Spike loss 0.7087\n",
      "Iter.   90 | Performance 0.0024 | Loss 0.0034 | Spike loss 1.0294\n",
      "Iter.  120 | Performance 0.0113 | Loss 0.0031 | Spike loss 1.0199\n",
      "Iter.  150 | Performance -0.0777 | Loss 0.0031 | Spike loss 1.0035\n",
      "Iter.  180 | Performance -0.0319 | Loss 0.0030 | Spike loss 0.9853\n",
      "Iter.  210 | Performance -0.0700 | Loss 0.0029 | Spike loss 0.9601\n",
      "Iter.  240 | Performance 0.0225 | Loss 0.0028 | Spike loss 0.9438\n",
      "Iter.  270 | Performance 0.0884 | Loss 0.0027 | Spike loss 0.9153\n",
      "Iter.  300 | Performance -0.0978 | Loss 0.0027 | Spike loss 0.8941\n",
      "Iter.  330 | Performance 0.0757 | Loss 0.0025 | Spike loss 0.8825\n",
      "Iter.  360 | Performance -0.0133 | Loss 0.0026 | Spike loss 0.8780\n",
      "Iter.  390 | Performance 0.0413 | Loss 0.0025 | Spike loss 0.8649\n",
      "Iter.  420 | Performance 0.0871 | Loss 0.0025 | Spike loss 0.8555\n",
      "Iter.  450 | Performance -0.0217 | Loss 0.0025 | Spike loss 0.8486\n",
      "Iter.  480 | Performance -0.0393 | Loss 0.0026 | Spike loss 0.8346\n",
      "Iter.  510 | Performance 0.0206 | Loss 0.0025 | Spike loss 0.8208\n",
      "Iter.  540 | Performance -0.0921 | Loss 0.0026 | Spike loss 0.8107\n",
      "Iter.  570 | Performance -0.0140 | Loss 0.0025 | Spike loss 0.8050\n",
      "Iter.  600 | Performance -0.0162 | Loss 0.0025 | Spike loss 0.7909\n",
      "Iter.  630 | Performance 0.0993 | Loss 0.0025 | Spike loss 0.7823\n",
      "Iter.  660 | Performance -0.0033 | Loss 0.0025 | Spike loss 0.7771\n",
      "Iter.  690 | Performance -0.0418 | Loss 0.0025 | Spike loss 0.7698\n",
      "Iter.  720 | Performance 0.0314 | Loss 0.0025 | Spike loss 0.7655\n",
      "Iter.  750 | Performance 0.0198 | Loss 0.0025 | Spike loss 0.7606\n",
      "Iter.  780 | Performance 0.0709 | Loss 0.0025 | Spike loss 0.7570\n",
      "Iter.  810 | Performance 0.0108 | Loss 0.0025 | Spike loss 0.7558\n",
      "Iter.  840 | Performance -0.0307 | Loss 0.0025 | Spike loss 0.7655\n",
      "Iter.  870 | Performance 0.0372 | Loss 0.0025 | Spike loss 0.7682\n",
      "Iter.  900 | Performance 0.0130 | Loss 0.0024 | Spike loss 0.7851\n",
      "Iter.  930 | Performance 0.0526 | Loss 0.0024 | Spike loss 0.8269\n",
      "Iter.  960 | Performance 0.2320 | Loss 0.0024 | Spike loss 0.9298\n",
      "Iter.  990 | Performance 0.2673 | Loss 0.0023 | Spike loss 1.4263\n",
      "Iter. 1020 | Performance 0.5342 | Loss 0.0021 | Spike loss 1.7092\n",
      "Iter. 1050 | Performance 0.6534 | Loss 0.0018 | Spike loss 2.3330\n",
      "Iter. 1080 | Performance 0.5966 | Loss 0.0018 | Spike loss 1.9040\n",
      "Iter. 1110 | Performance 0.4308 | Loss 0.0019 | Spike loss 1.8607\n",
      "Iter. 1140 | Performance 0.6543 | Loss 0.0017 | Spike loss 2.3733\n",
      "Iter. 1170 | Performance 0.5834 | Loss 0.0017 | Spike loss 1.9491\n",
      "Iter. 1200 | Performance 0.5832 | Loss 0.0017 | Spike loss 2.0204\n",
      "Iter. 1230 | Performance 0.5857 | Loss 0.0018 | Spike loss 3.2283\n",
      "Iter. 1260 | Performance 0.6558 | Loss 0.0016 | Spike loss 3.3247\n",
      "Iter. 1290 | Performance 0.5639 | Loss 0.0017 | Spike loss 3.0678\n",
      "Iter. 1320 | Performance 0.6248 | Loss 0.0016 | Spike loss 2.9674\n",
      "Iter. 1350 | Performance 0.5705 | Loss 0.0017 | Spike loss 2.7715\n",
      "Iter. 1380 | Performance 0.6222 | Loss 0.0016 | Spike loss 2.6286\n",
      "Iter. 1410 | Performance 0.5771 | Loss 0.0016 | Spike loss 2.6990\n",
      "Iter. 1440 | Performance 0.6150 | Loss 0.0016 | Spike loss 2.5359\n",
      "Iter. 1470 | Performance 0.6255 | Loss 0.0015 | Spike loss 2.5276\n",
      "Iter. 1500 | Performance 0.6183 | Loss 0.0015 | Spike loss 2.5334\n",
      "Iter. 1530 | Performance 0.6095 | Loss 0.0016 | Spike loss 2.5847\n",
      "Iter. 1560 | Performance 0.5751 | Loss 0.0015 | Spike loss 2.6707\n",
      "Iter. 1590 | Performance 0.6585 | Loss 0.0013 | Spike loss 2.7507\n",
      "Iter. 1620 | Performance 0.5862 | Loss 0.0015 | Spike loss 2.6653\n",
      "Iter. 1650 | Performance 0.6626 | Loss 0.0014 | Spike loss 2.7919\n",
      "Iter. 1680 | Performance 0.6391 | Loss 0.0014 | Spike loss 2.6895\n",
      "Iter. 1710 | Performance 0.6343 | Loss 0.0014 | Spike loss 2.6506\n",
      "Iter. 1740 | Performance 0.7834 | Loss 0.0012 | Spike loss 2.6880\n",
      "Iter. 1770 | Performance 0.6787 | Loss 0.0014 | Spike loss 2.6324\n",
      "Iter. 1800 | Performance 0.7323 | Loss 0.0013 | Spike loss 2.7640\n",
      "Iter. 1830 | Performance 0.7909 | Loss 0.0012 | Spike loss 3.1260\n",
      "Iter. 1860 | Performance 0.7875 | Loss 0.0012 | Spike loss 2.9187\n",
      "Iter. 1890 | Performance 0.8311 | Loss 0.0011 | Spike loss 2.7642\n",
      "Iter. 1920 | Performance 0.8794 | Loss 0.0009 | Spike loss 3.0088\n",
      "Iter. 1950 | Performance 0.8755 | Loss 0.0010 | Spike loss 3.7911\n",
      "Iter. 1980 | Performance 0.9363 | Loss 0.0006 | Spike loss 3.4970\n",
      "Iter. 2010 | Performance 0.9236 | Loss 0.0007 | Spike loss 4.1991\n",
      "Iter. 2040 | Performance 0.8619 | Loss 0.0011 | Spike loss 5.5627\n",
      "Iter. 2070 | Performance 0.9042 | Loss 0.0007 | Spike loss 3.6880\n",
      "Iter. 2100 | Performance 0.9089 | Loss 0.0007 | Spike loss 3.4607\n",
      "Iter. 2130 | Performance 0.9260 | Loss 0.0006 | Spike loss 4.3420\n",
      "Iter. 2160 | Performance 0.8914 | Loss 0.0009 | Spike loss 5.1553\n",
      "Iter. 2190 | Performance 0.9094 | Loss 0.0009 | Spike loss 5.6762\n",
      "Iter. 2220 | Performance 0.9305 | Loss 0.0005 | Spike loss 3.7175\n",
      "Iter. 2250 | Performance 0.9331 | Loss 0.0005 | Spike loss 3.7785\n",
      "Iter. 2280 | Performance 0.9214 | Loss 0.0006 | Spike loss 3.5340\n",
      "Iter. 2310 | Performance 0.9235 | Loss 0.0006 | Spike loss 3.6374\n",
      "Iter. 2340 | Performance 0.9358 | Loss 0.0005 | Spike loss 3.6694\n",
      "Iter. 2370 | Performance 0.9445 | Loss 0.0005 | Spike loss 3.8860\n",
      "Iter. 2400 | Performance 0.9357 | Loss 0.0006 | Spike loss 4.0366\n",
      "Iter. 2430 | Performance 0.9336 | Loss 0.0006 | Spike loss 3.9313\n",
      "Iter. 2460 | Performance 0.9169 | Loss 0.0006 | Spike loss 3.2835\n",
      "Iter. 2490 | Performance 0.9280 | Loss 0.0005 | Spike loss 3.6148\n",
      "Iter. 2520 | Performance 0.9406 | Loss 0.0005 | Spike loss 3.7753\n",
      "Iter. 2550 | Performance 0.9296 | Loss 0.0005 | Spike loss 3.3019\n",
      "Iter. 2580 | Performance 0.8989 | Loss 0.0006 | Spike loss 3.3747\n",
      "Iter. 2610 | Performance 0.9294 | Loss 0.0006 | Spike loss 3.2169\n",
      "Iter. 2640 | Performance 0.7736 | Loss 0.0012 | Spike loss 2.7671\n",
      "Iter. 2670 | Performance 0.1085 | Loss 0.0033 | Spike loss 2.2403\n",
      "Iter. 2700 | Performance 0.0573 | Loss 0.0024 | Spike loss 2.3161\n",
      "Iter. 2730 | Performance 0.0937 | Loss 0.0023 | Spike loss 2.3108\n",
      "Iter. 2760 | Performance 0.2011 | Loss 0.0023 | Spike loss 2.3190\n",
      "Iter. 2790 | Performance -0.0309 | Loss 0.0024 | Spike loss 2.3086\n",
      "Iter. 2820 | Performance 0.1518 | Loss 0.0023 | Spike loss 2.2070\n",
      "Iter. 2850 | Performance 0.2537 | Loss 0.0023 | Spike loss 2.1906\n",
      "Iter. 2880 | Performance 0.4458 | Loss 0.0022 | Spike loss 2.2364\n",
      "Iter. 2910 | Performance 0.5669 | Loss 0.0019 | Spike loss 2.2946\n",
      "Iter. 2940 | Performance 0.6422 | Loss 0.0017 | Spike loss 2.4736\n",
      "Iter. 2970 | Performance 0.6657 | Loss 0.0015 | Spike loss 2.7971\n",
      "Iter. 3000 | Performance 0.6786 | Loss 0.0014 | Spike loss 2.8026\n",
      "Iter. 3030 | Performance 0.8058 | Loss 0.0011 | Spike loss 3.4164\n",
      "Iter. 3060 | Performance 0.8179 | Loss 0.0010 | Spike loss 3.4635\n",
      "Iter. 3090 | Performance 0.7876 | Loss 0.0010 | Spike loss 3.4471\n",
      "Iter. 3120 | Performance 0.8068 | Loss 0.0010 | Spike loss 3.4548\n",
      "Iter. 3150 | Performance 0.7985 | Loss 0.0010 | Spike loss 3.3600\n",
      "Iter. 3180 | Performance 0.8653 | Loss 0.0009 | Spike loss 3.6616\n",
      "Iter. 3210 | Performance 0.8359 | Loss 0.0009 | Spike loss 3.5578\n",
      "Iter. 3240 | Performance 0.8449 | Loss 0.0009 | Spike loss 3.5827\n",
      "Iter. 3270 | Performance 0.8385 | Loss 0.0009 | Spike loss 3.7373\n",
      "Iter. 3300 | Performance 0.8482 | Loss 0.0009 | Spike loss 3.4694\n",
      "Iter. 3330 | Performance 0.8423 | Loss 0.0009 | Spike loss 3.6160\n",
      "Iter. 3360 | Performance 0.8029 | Loss 0.0010 | Spike loss 3.1820\n",
      "Iter. 3390 | Performance 0.8871 | Loss 0.0008 | Spike loss 3.9431\n",
      "Iter. 3420 | Performance 0.9314 | Loss 0.0006 | Spike loss 4.3908\n",
      "Iter. 3450 | Performance 0.9113 | Loss 0.0007 | Spike loss 4.1875\n",
      "Iter. 3480 | Performance 0.8875 | Loss 0.0007 | Spike loss 3.8184\n",
      "Iter. 3510 | Performance 0.9217 | Loss 0.0006 | Spike loss 4.5035\n",
      "Iter. 3540 | Performance 0.8691 | Loss 0.0008 | Spike loss 3.7995\n",
      "Iter. 3570 | Performance 0.9116 | Loss 0.0006 | Spike loss 3.7582\n",
      "Iter. 3600 | Performance 0.7888 | Loss 0.0010 | Spike loss 3.7262\n",
      "Iter. 3630 | Performance 0.8513 | Loss 0.0009 | Spike loss 3.6109\n",
      "Iter. 3660 | Performance 0.8654 | Loss 0.0008 | Spike loss 3.5733\n",
      "Iter. 3690 | Performance 0.9027 | Loss 0.0007 | Spike loss 4.1389\n",
      "Iter. 3720 | Performance 0.9035 | Loss 0.0007 | Spike loss 4.2386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter. 3750 | Performance 0.9145 | Loss 0.0007 | Spike loss 5.5845\n",
      "Iter. 3780 | Performance 0.9195 | Loss 0.0007 | Spike loss 6.6774\n",
      "Iter. 3810 | Performance 0.9041 | Loss 0.0007 | Spike loss 6.3911\n",
      "Iter. 3840 | Performance 0.9325 | Loss 0.0006 | Spike loss 6.6461\n",
      "Iter. 3870 | Performance 0.9379 | Loss 0.0007 | Spike loss 6.6841\n",
      "Iter. 3900 | Performance 0.9345 | Loss 0.0006 | Spike loss 6.9670\n",
      "Iter. 3930 | Performance 0.9374 | Loss 0.0006 | Spike loss 6.0139\n",
      "Iter. 3960 | Performance 0.9270 | Loss 0.0007 | Spike loss 5.9826\n",
      "Iter. 3990 | Performance 0.9301 | Loss 0.0007 | Spike loss 5.9967\n",
      "Iter. 4020 | Performance 0.9290 | Loss 0.0007 | Spike loss 5.5465\n",
      "Iter. 4050 | Performance 0.9229 | Loss 0.0007 | Spike loss 5.7685\n",
      "Iter. 4080 | Performance 0.9360 | Loss 0.0006 | Spike loss 6.1586\n",
      "Iter. 4110 | Performance 0.9274 | Loss 0.0007 | Spike loss 5.9451\n",
      "Iter. 4140 | Performance 0.9364 | Loss 0.0006 | Spike loss 5.6158\n",
      "Iter. 4170 | Performance 0.9444 | Loss 0.0005 | Spike loss 5.6624\n",
      "Iter. 4200 | Performance 0.9265 | Loss 0.0006 | Spike loss 5.5490\n",
      "Iter. 4230 | Performance 0.9330 | Loss 0.0006 | Spike loss 5.1544\n",
      "Iter. 4260 | Performance 0.9279 | Loss 0.0006 | Spike loss 5.9061\n",
      "Iter. 4290 | Performance 0.9374 | Loss 0.0006 | Spike loss 5.3168\n",
      "Iter. 4320 | Performance 0.9421 | Loss 0.0006 | Spike loss 5.1317\n",
      "Iter. 4350 | Performance 0.8053 | Loss 0.0012 | Spike loss 4.1240\n",
      "Iter. 4380 | Performance 0.7864 | Loss 0.0010 | Spike loss 3.2086\n",
      "Iter. 4410 | Performance 0.8561 | Loss 0.0008 | Spike loss 3.5234\n",
      "Iter. 4440 | Performance 0.8236 | Loss 0.0009 | Spike loss 3.2130\n",
      "Iter. 4470 | Performance 0.8285 | Loss 0.0009 | Spike loss 3.4001\n",
      "Iter. 4500 | Performance 0.8772 | Loss 0.0008 | Spike loss 3.4434\n",
      "Iter. 4530 | Performance 0.8570 | Loss 0.0008 | Spike loss 3.4196\n",
      "Iter. 4560 | Performance 0.8436 | Loss 0.0008 | Spike loss 3.4561\n",
      "Iter. 4590 | Performance 0.8294 | Loss 0.0008 | Spike loss 3.3201\n",
      "Iter. 4620 | Performance 0.8655 | Loss 0.0008 | Spike loss 3.2246\n",
      "Iter. 4650 | Performance 0.8440 | Loss 0.0008 | Spike loss 3.4164\n",
      "Iter. 4680 | Performance 0.8556 | Loss 0.0008 | Spike loss 3.2264\n",
      "Iter. 4710 | Performance 0.8401 | Loss 0.0009 | Spike loss 3.1062\n",
      "Iter. 4740 | Performance 0.8389 | Loss 0.0009 | Spike loss 3.2762\n",
      "Iter. 4770 | Performance 0.8520 | Loss 0.0008 | Spike loss 3.4680\n",
      "Iter. 4800 | Performance 0.8365 | Loss 0.0009 | Spike loss 3.0901\n",
      "Iter. 4830 | Performance 0.8618 | Loss 0.0007 | Spike loss 3.3891\n",
      "Iter. 4860 | Performance 0.8777 | Loss 0.0007 | Spike loss 3.3271\n",
      "Iter. 4890 | Performance 0.8845 | Loss 0.0008 | Spike loss 3.4187\n",
      "Iter. 4920 | Performance 0.8773 | Loss 0.0007 | Spike loss 3.2693\n",
      "Iter. 4950 | Performance 0.8810 | Loss 0.0008 | Spike loss 3.2371\n",
      "Iter. 4980 | Performance 0.8599 | Loss 0.0008 | Spike loss 3.3788\n",
      "Iter. 5010 | Performance 0.9068 | Loss 0.0006 | Spike loss 3.3512\n",
      "Iter. 5040 | Performance 0.8942 | Loss 0.0007 | Spike loss 3.1488\n",
      "Iter. 5070 | Performance 0.9168 | Loss 0.0006 | Spike loss 3.5029\n",
      "Iter. 5100 | Performance 0.9313 | Loss 0.0006 | Spike loss 3.3393\n",
      "Iter. 5130 | Performance 0.9315 | Loss 0.0006 | Spike loss 3.3117\n",
      "Iter. 5160 | Performance 0.9404 | Loss 0.0005 | Spike loss 3.3885\n",
      "Iter. 5190 | Performance 0.9328 | Loss 0.0005 | Spike loss 3.3872\n",
      "Iter. 5220 | Performance 0.9419 | Loss 0.0004 | Spike loss 3.5784\n",
      "Iter. 5250 | Performance 0.8908 | Loss 0.0013 | Spike loss 4.6814\n",
      "Iter. 5280 | Performance 0.8587 | Loss 0.0010 | Spike loss 4.2605\n",
      "Iter. 5310 | Performance 0.9304 | Loss 0.0007 | Spike loss 4.6601\n",
      "Iter. 5340 | Performance 0.9198 | Loss 0.0007 | Spike loss 4.2102\n",
      "Iter. 5370 | Performance 0.9294 | Loss 0.0006 | Spike loss 4.3453\n",
      "Iter. 5400 | Performance 0.9294 | Loss 0.0006 | Spike loss 4.3249\n",
      "Iter. 5430 | Performance 0.9330 | Loss 0.0006 | Spike loss 4.2462\n",
      "Iter. 5460 | Performance 0.9176 | Loss 0.0007 | Spike loss 4.0614\n",
      "Iter. 5490 | Performance 0.9482 | Loss 0.0006 | Spike loss 4.5591\n",
      "Iter. 5520 | Performance 0.9193 | Loss 0.0007 | Spike loss 4.4681\n",
      "Iter. 5550 | Performance 0.9363 | Loss 0.0006 | Spike loss 4.4502\n",
      "Iter. 5580 | Performance 0.9409 | Loss 0.0006 | Spike loss 4.0034\n",
      "Iter. 5610 | Performance 0.9503 | Loss 0.0006 | Spike loss 4.3749\n",
      "Iter. 5640 | Performance 0.9519 | Loss 0.0005 | Spike loss 3.9744\n",
      "Iter. 5670 | Performance 0.9348 | Loss 0.0005 | Spike loss 4.3402\n",
      "Iter. 5700 | Performance 0.9592 | Loss 0.0004 | Spike loss 4.0297\n",
      "INFO:tensorflow:Assets written to: D:\\proj\\det_rnn\\outputÂ€621_test/model_level0\\assets\n",
      "################################################################################\n",
      "Criterion satisfied!(Time Spent: 5609.95s)\tNow extending: 1.5\n",
      "################################################################################\n",
      "Iter. 5730 | Performance 0.9421 | Loss 0.0005 | Spike loss 3.5797\n",
      "Iter. 5760 | Performance 0.9300 | Loss 0.0005 | Spike loss 3.3960\n",
      "Iter. 5790 | Performance 0.9216 | Loss 0.0005 | Spike loss 3.6940\n",
      "Iter. 5820 | Performance 0.8981 | Loss 0.0005 | Spike loss 3.4537\n",
      "Iter. 5850 | Performance 0.8709 | Loss 0.0009 | Spike loss 3.9121\n",
      "Iter. 5880 | Performance 0.8724 | Loss 0.0008 | Spike loss 3.9498\n",
      "Iter. 5910 | Performance 0.8782 | Loss 0.0007 | Spike loss 3.8198\n",
      "Iter. 5940 | Performance 0.8666 | Loss 0.0007 | Spike loss 3.6046\n",
      "Iter. 5970 | Performance 0.8644 | Loss 0.0007 | Spike loss 3.3370\n",
      "Iter. 6000 | Performance 0.9036 | Loss 0.0006 | Spike loss 3.2987\n",
      "Iter. 6030 | Performance 0.9195 | Loss 0.0005 | Spike loss 3.1786\n",
      "Iter. 6060 | Performance 0.9202 | Loss 0.0005 | Spike loss 3.4213\n",
      "Iter. 6090 | Performance 0.9382 | Loss 0.0005 | Spike loss 3.2532\n",
      "Iter. 6120 | Performance 0.9243 | Loss 0.0005 | Spike loss 3.3740\n",
      "Iter. 6150 | Performance 0.9374 | Loss 0.0005 | Spike loss 4.1992\n",
      "Iter. 6180 | Performance 0.9513 | Loss 0.0004 | Spike loss 4.3569\n",
      "Iter. 6210 | Performance 0.9438 | Loss 0.0004 | Spike loss 4.0609\n",
      "Iter. 6240 | Performance 0.9572 | Loss 0.0004 | Spike loss 4.0571\n",
      "Iter. 6270 | Performance 0.9401 | Loss 0.0004 | Spike loss 3.9520\n",
      "Iter. 6300 | Performance 0.9427 | Loss 0.0004 | Spike loss 3.6532\n"
     ]
    }
   ],
   "source": [
    "# boost_rnn\n",
    "par = update_parameters(par)\n",
    "stimulus = Stimulus()\n",
    "ti_spec = dt.gen_ti_spec(stimulus.generate_trial())\n",
    "\n",
    "model_performance = {'perf': [], 'loss': [], 'perf_loss': [], 'spike_loss': []}\n",
    "\n",
    "# Boosting RNN\n",
    "N_boost_max = 100000\n",
    "perf_crit   = 0.95 # Human mean performance level\n",
    "recency     = 50   # Number of 'recent' epochs to be assayed\n",
    "boost_step  = 1.5  # How much step should we increase\n",
    "\n",
    "extend_time = np.arange(boost_step,15.5,step=boost_step)\n",
    "mileage_lim = len(extend_time)\n",
    "milestones  = np.zeros((mileage_lim,), dtype=np.int64)\n",
    "timestones  = np.zeros((mileage_lim,))\n",
    "\n",
    "# Start boosting\n",
    "model = dt.initialize_rnn(ti_spec) # initialize RNN to be boosted\n",
    "\n",
    "mileage = -1\n",
    "start_time = time.time()\n",
    "print(\"RNN Booster started!\")\n",
    "for iter in range(N_boost_max):\n",
    "    trial_info = dt.tensorize_trial(stimulus.generate_trial())\n",
    "    Y, Loss = model(trial_info, dt.hp)\n",
    "    model_performance = dt.append_model_performance(model_performance, trial_info, Y, Loss, par)\n",
    "\n",
    "    if iter % 30 == 0:\n",
    "        dt.print_results(model_performance, iter)\n",
    "\n",
    "    if  dt.level_up_criterion(iter,perf_crit,recency,milestones[mileage],model_performance):\n",
    "        check_time = time.time()\n",
    "        mileage += 1\n",
    "        if mileage >= mileage_lim:\n",
    "            print(\"#\"*80+\"\\nTraining criterion finally met!(Time Spent: {:0.2f}s)\\t\".format(check_time-start_time)+\n",
    "                  \"Now climb down the mountain!\\n\"+\"#\"*80)\n",
    "            break\n",
    "        milestones[mileage] = iter\n",
    "        timestones[mileage] = check_time-start_time\n",
    "\n",
    "        ## Attach to the model\n",
    "        model.model_performance = dt.tensorize_model_performance(model_performance)\n",
    "        model.milestones = tf.Variable(milestones, trainable=False)\n",
    "        model.timestones = tf.Variable(timestones, trainable=False)\n",
    "\n",
    "        ## save the model\n",
    "        os.makedirs(model_dir + \"/model_level\" + str(mileage), exist_ok=True)\n",
    "        tf.saved_model.save(model, model_dir  + \"/model_level\" + str(mileage))\n",
    "\n",
    "        ## upgrade to higher level\n",
    "        par['design'].update({'iti': (0, 1.5),\n",
    "                              'stim': (1.5, 3.0),\n",
    "                              'delay': (3.0, 4.5 + extend_time[mileage]),\n",
    "                              'estim': (4.5 + extend_time[mileage], 6.0 + extend_time[mileage])})\n",
    "        par = update_parameters(par)\n",
    "        stimulus = Stimulus(par)\n",
    "\n",
    "        ## modulate hyperparameters #######################################################\n",
    "        # dt.hp['spike_cost'] /= 2.\n",
    "        ###################################################################################\n",
    "\n",
    "        ## Report an upgrade has been performed\n",
    "        print(\"#\"*80+\"\\nCriterion satisfied!(Time Spent: {:0.2f}s)\\t\".format(check_time-start_time)+\n",
    "                     \"Now extending: {:0.1f}\\n\".format(extend_time[mileage])+\"#\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detrnn37",
   "language": "python",
   "name": "detrnn37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
